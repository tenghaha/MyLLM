import hashlib
import json
import time
from typing import Sequence
from typing_extensions import Annotated, TypedDict

import streamlit as st

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, END, MessagesState, StateGraph
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langchain_deepseek import ChatDeepSeek
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder



# TODO: å¾…å®ç°åŠŸèƒ½ï¼šPromptå‚¨å­˜ã€ç”¨æˆ·è®¤è¯ã€RAGã€Agent Toolsã€OpenAIå…¼å®¹
# é¡µé¢å¼€å§‹
st.header("Prompts")
st.caption("ğŸ’¬ç›´æ¥å¼€å§‹æˆ–ä½¿ç”¨é¢„å®šçš„promptæ¨¡æ¿è¿›è¡Œä¼šè¯")
st.caption("âš™ï¸åœ¨å·¦ä¾§è¾¹æ è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œå³ä¾§ä¸»çª—å£è¿›è¡Œä¼šè¯/ä¿å­˜æ¨¡æ¿/å¯¼å‡ºä¼šè¯è®°å½•ç­‰")

# 1. ä¾§è¾¹æ ï¼šç¡®å®šæ¨¡å‹åŠå‚æ•°
def hash_from_time():
    time_bytes = str(time.time()).encode("utf-8")
    return hashlib.md5(time_bytes).hexdigest()

def restart_session():
    st.session_state["model_config"]["thread_id"] = hash_from_time()
    st.session_state["messages"] = []
    st.session_state["confirmed_restart"] = True
    st.rerun()

@st.dialog("æ³¨æ„")
def if_restart():
    st.write("å¼€å§‹ä¸€æ®µæ–°çš„å¯¹è¯å¹¶æ¸…ç©ºæ‰€æœ‰å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œç¡®è®¤å—ï¼Ÿ")
    c1, c2 = st.columns(2, vertical_alignment="bottom")
    with c1:
        if st.button("ç¡®è®¤", use_container_width=True):
            restart_session()
    with c2:
        if st.button("å–æ¶ˆ", use_container_width=True):
            st.rerun()       


with st.sidebar:
    if "model_config" not in st.session_state:
        st.session_state["model_config"] = {"thread_id": hash_from_time()}
    
    st.session_state.model_config["api"] = st.selectbox(
        "API",
        options=["DeepSeek"],
        placeholder="é€‰æ‹©APIæä¾›è€…"
    )
    #TODO: è‡ªåŠ¨é€‰æ‹©æ¨¡å‹
    st.session_state.model_config["model"] = st.selectbox(
        "æ¨¡å‹",
        options=["deepseek-chat", "deepseek-reasoner"],
        placeholder="é€‰æ‹©æ¨¡å‹"
    )

    # st.info(st.session_state.model_config["model"])

    st.session_state.model_config["response_type"] = st.radio(
        "è¾“å‡ºæ ¼å¼",
        options=["text","json_mode"],
        captions=["ä»¥çº¯æ–‡æœ¬æ ¼å¼è¾“å‡º","ä»¥JSONæ ¼å¼è¾“å‡º"]
    )
    st.session_state.model_config["max_tokens"] = st.slider(
        "max_tokens",
        1, 8192,
        value=4096,
        step=32,
        help="é™åˆ¶ä¸€æ¬¡è¯·æ±‚ä¸­æ¨¡å‹ç”Ÿæˆ completion çš„æœ€å¤§ token æ•°ã€‚è¾“å…¥ token å’Œè¾“å‡º token çš„æ€»é•¿åº¦å—æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦çš„é™åˆ¶ã€‚"
    )
    st.session_state.model_config["temperature"] = st.slider(
        "temperature",
        0., 2.,
        value=1.,
        step=0.01,
        help="é‡‡æ ·æ¸©åº¦ã€‚æ›´é«˜çš„å€¼ä¼šä½¿è¾“å‡ºæ›´éšæœºï¼Œè€Œæ›´ä½çš„å€¼ä¼šä½¿å…¶æ›´åŠ é›†ä¸­å’Œç¡®å®šã€‚"
    )
    st.session_state.model_config["top_p"] = st.slider(
        "top_p",
        0., 1.,
        value=1.,
        step=0.01,
        help="ä½œä¸ºè°ƒèŠ‚é‡‡æ ·æ¸©åº¦çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ¨¡å‹ä¼šè€ƒè™‘å‰ top_p æ¦‚ç‡çš„ token çš„ç»“æœã€‚ä¸å»ºè®®ä¸ temperature åŒæ—¶ä¿®æ”¹ã€‚"
    )
    st.session_state.model_config["presence_penalty"] = st.slider(
        "presence_penalty",
        -2., 2.,
        value=0.,
        step=0.1,
        help="å¦‚æœè¯¥å€¼ä¸ºæ­£ï¼Œé‚£ä¹ˆæ–° token ä¼šæ ¹æ®å…¶æ˜¯å¦å·²åœ¨å·²æœ‰æ–‡æœ¬ä¸­å‡ºç°å—åˆ°ç›¸åº”çš„æƒ©ç½šï¼Œä»è€Œå¢åŠ æ¨¡å‹è°ˆè®ºæ–°ä¸»é¢˜çš„å¯èƒ½æ€§ã€‚"
    )
    st.session_state.model_config["frequency_penalty"] = st.slider(
        "frequency_penalty",
        -2., 2.,
        value=0.,
        step=0.1,
        help="ä»‹äº -2.0 å’Œ 2.0 ä¹‹é—´çš„æ•°å­—ã€‚å¦‚æœè¯¥å€¼ä¸ºæ­£ï¼Œé‚£ä¹ˆæ–° token ä¼šæ ¹æ®å…¶åœ¨å·²æœ‰æ–‡æœ¬ä¸­çš„å‡ºç°é¢‘ç‡å—åˆ°ç›¸åº”çš„æƒ©ç½šï¼Œé™ä½æ¨¡å‹é‡å¤ç›¸åŒå†…å®¹çš„å¯èƒ½æ€§ã€‚"
    )


# 2. é¢„å¤„ç†ï¼šç¼–è¯‘LangGraph

class State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    system_prompt: str

class ConfigSchema(TypedDict):
    model: str
    max_tokens: int
    temperature: float
    top_p: float
    frequency_penalty: float
    presence_penalty: float

def call_model(state: State, config: RunnableConfig):
    model = ChatDeepSeek(
        # base_url=BASE_URL[st.session_state.model_config["api"]],
        model_name=config["configurable"].get("model"),
        max_tokens=config["configurable"].get("max_tokens"),
        temperature=config["configurable"].get("temperature"),
        top_p=config["configurable"].get("top_p"),
        frequency_penalty=config["configurable"].get("frequency_penalty"),
        presence_penalty=config["configurable"].get("presence_penalty")
    )
    prompt_template = ChatPromptTemplate.from_messages(
        [
            ("system", "{system_prompt}"),
            MessagesPlaceholder(variable_name="messages"),
        ]
    )
    prompt = prompt_template.invoke(state)
    if config["configurable"].get("response_type") == "json_mode":
        response = model.with_structured_output(
            method='json_mode',
            include_raw=True
            ).invoke(prompt)["raw"] # å–åŸå§‹JSONå­—ç¬¦ä¸²message
    else:
        response = model.invoke(prompt)
    return {"messages": [response]}

workflow = StateGraph(state_schema=State, config_schema=ConfigSchema)
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)
workflow.add_edge("model", END)

memory = MemorySaver()
if "app" not in st.session_state:
    st.session_state["app"] = workflow.compile(checkpointer=memory)


# 3. ä¸»é¡µé¢ï¼šå¤„ç†promptå’Œè¾“å…¥
class OutputWrapper():
    def __init__(self):
        self._cot_end = False
        self.values = {}
        self.sp = "------------------------------------"

    def __call__(self, output):
        for stream_mode, chunk in output:
            if stream_mode == "messages":
                msg_chunk, _ = chunk
                if "reasoning_content" not in msg_chunk.additional_kwargs and not msg_chunk.content:
                    yield ""
                elif "reasoning_content" in msg_chunk.additional_kwargs:
                    yield msg_chunk.additional_kwargs["reasoning_content"]
                else:
                    if not self._cot_end:
                        self._cot_end = True
                        yield f"\n\n{self.sp}\n\n"
                    yield msg_chunk.content
            else:
                self.values = chunk


@st.fragment()
def run_chat(output):
    output_wrapper = OutputWrapper()
    msg_placeholder = st.empty()
    with msg_placeholder:
        with st.status("æ€è€ƒä¸­...", expanded=True):
            msg = st.write_stream(output_wrapper(output))
    msg_placeholder.empty()
    cot_msg, msg = msg.split(output_wrapper.sp)
    cot_msg, msg = cot_msg.strip(), msg.strip()
    if cot_msg:
        with st.expander("æ€ç»´é“¾"):
            st.caption(cot_msg)
    st.markdown(msg)
    st.session_state["messages"] = output_wrapper.values["messages"]


@st.dialog("ä¿å­˜æ¨¡æ¿")
def save_system_prompt(system_prompt):
    if not system_prompt:
        st.write("è¦ä¿å­˜çš„promptæ¨¡æ¿ä¸å¯ä¸ºç©º")
        if st.button("ç¡®å®š", use_container_width=True):
            st.rerun()
    else:
        prompt_name = st.text_input("æ¨¡æ¿åç§°", max_chars=20)
        prompt_desc = st.text_input("æ¨¡æ¿æè¿°ï¼ˆå¯é€‰ï¼‰", max_chars=100)
        confirm_col, cancel_col = st.columns(2)
        if confirm_col.button("ä¿å­˜", use_container_width=True):
            with open("prompts.json", encoding="utf-8") as fp:
                prompt_dict = json.loads(fp.read())
            with open("prompts.json", "w", encoding="utf-8") as fp:
                if prompt_name in prompt_dict:
                    prompt_name = prompt_name + "_1"    # æ¨¡æ¿é‡åå¤„ç†
                prompt_dict[prompt_name] = {
                    "content": system_prompt,
                    "description": prompt_desc
                }
                fp.write(json.dumps(prompt_dict, ensure_ascii=False))
            st.rerun()
        if cancel_col.button("å–æ¶ˆ", use_container_width=True):
            st.rerun()


@st.dialog("æ›´æ–°æ¨¡æ¿")
def update_system_prompt(prompt_name ,system_prompt):
    if not system_prompt:
        st.write("è¦ä¿å­˜çš„promptæ¨¡æ¿ä¸å¯ä¸ºç©º")
        if st.button("ç¡®å®š"):
            st.rerun()
    else:
        st.write("æ˜¯å¦ä¿å­˜æ–°çš„æ¨¡æ¿ï¼Ÿ")
        prompt_desc = st.text_input("æ¨¡æ¿æè¿°ï¼ˆç•™ç©ºåˆ™ä¸å˜ï¼‰", max_chars=100)
        confirm_col, cancel_col = st.columns(2)
        if confirm_col.button("ä¿å­˜", use_container_width=True):
            with open("prompts.json", encoding="utf-8") as fp:
                prompt_dict = json.loads(fp.read())
            with open("prompts.json", "w", encoding="utf-8") as fp:
                prompt_dict[prompt_name]["content"] = system_prompt
                prompt_dict[prompt_name]["description"] = prompt_desc
                fp.write(json.dumps(prompt_dict, ensure_ascii=False))
            st.rerun()
        if cancel_col.button("å–æ¶ˆ", use_container_width=True):
            st.rerun()


@st.dialog("åˆ é™¤æ¨¡æ¿")
def delete_system_prompt(prompt_name):
    st.write("æ˜¯å¦ç¡®è®¤åˆ é™¤ï¼Ÿ :red[æœ¬æ“ä½œä¸å¯æ’¤é”€ï¼]")
    confirm_col, cancel_col = st.columns(2)
    if confirm_col.button("ç¡®è®¤", use_container_width=True):
        with open("prompts.json", encoding="utf-8") as fp:
            prompt_dict = json.loads(fp.read())
        with open("prompts.json", "w", encoding="utf-8") as fp:
            prompt_dict.pop(prompt_name)
            fp.write(json.dumps(prompt_dict, ensure_ascii=False))
        st.rerun()
    if cancel_col.button("å–æ¶ˆ", use_container_width=True):
        st.rerun()


def parse_messages_history(_messages: list):
    parsed_messages = []
    for msg in _messages:
        if isinstance(msg, HumanMessage):
            parsed_messages.append({
                "role": "human",
                "content": msg.content
                })
        elif isinstance(msg, AIMessage):
            parsed_messages.append({
                "role": "ai",
                "content": msg.content,
                "additional_kwargs": msg.additional_kwargs  # TODO:å¦‚ä½•å¤„ç†tool call
                })
    return parsed_messages


if "messages" not in st.session_state:
    st.session_state["messages"] = []

prompt_col, main_col, widget_col = st.columns([0.35, 0.6, 0.05])

with prompt_col:    # TODO: ä¸ºæ¯ä¸ªç”¨æˆ·è®¾ç½®ä¸åŒçš„promptåº“
    with st.container(height=550):
        prompt_save_placeholder = st.container()
        prompt_content_placeholder = st.empty()
        prompt_desc_placeholder = st.empty()

        system_prompt = prompt_content_placeholder.text_area(
            "Promptæ¨¡æ¿",
            height=200,
            max_chars=1000,
            placeholder="æç¤ºéœ€è¦æ¨¡å‹åšçš„äº‹æƒ…\nä¾‹å¦‚ï¼šâ€œå¸®æˆ‘å°†è¾“å…¥çš„ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡â€"
        )
        with prompt_desc_placeholder.container(height=130):
            st.write("æ¨¡æ¿æè¿°")
            st.write()

        with open("prompts.json", encoding="utf-8") as fp:
            saved_prompts = json.loads(fp.read())
        prompt_name = prompt_save_placeholder.selectbox(
            "é€‰æ‹©æ¨¡æ¿",
            options=["(æ–°æ¨¡æ¿)"] + list(saved_prompts.keys())
        )
        if prompt_name == "(æ–°æ¨¡æ¿)":
            if prompt_save_placeholder.button("ä¿å­˜å½“å‰æ¨¡æ¿", icon=":material/save:", use_container_width=True):
                save_system_prompt(system_prompt)   # å¦‚æœé€‰æ‹©æ–°æ¨¡æ¿ï¼Œå¯ä»¥ä¿å­˜æ¨¡æ¿è‡³prompts.json
        else:
            system_prompt = prompt_content_placeholder.text_area(
                "Promptæ¨¡æ¿",
                value=saved_prompts[prompt_name]["content"],
                height=200,
                max_chars=1000,
                placeholder="æç¤ºéœ€è¦æ¨¡å‹åšçš„äº‹æƒ…\nä¾‹å¦‚ï¼šâ€œå¸®æˆ‘å°†è¾“å…¥çš„ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡â€"
            )

            prompt_desc_placeholder.text_area(
                "æ¨¡æ¿æè¿°",
                value=saved_prompts[prompt_name]["description"],
                height=100,
                disabled=True
                )

            p_col1, p_col2 = prompt_save_placeholder.columns(2)  # é€‰æ‹©å·²æœ‰æ¨¡æ¿çš„é€»è¾‘
            if p_col1.button("æ›´æ–°", icon=":material/sync:", use_container_width=True):
                update_system_prompt(prompt_name, system_prompt)
            if p_col2.button("åˆ é™¤", icon=":material/delete:", use_container_width=True):
                delete_system_prompt(prompt_name)


with main_col:
    chat_box = st.container(height=550)
    chatbox_placeholder = chat_box.empty()
    if not st.session_state.messages:
        chatbox_placeholder.caption("å‘é€æ¶ˆæ¯ä»¥å¼€å§‹èŠå¤©")
    for msg in st.session_state.messages:
        chat_box.chat_message(msg.type).write(msg.content)

with widget_col:
    if st.button("", icon=":material/restart_alt:", help="å¼€å§‹æ–°å¯¹è¯"):
        if_restart()
    parsed_messages = parse_messages_history(st.session_state.messages)
    parsed_messages = [{"role": "system", "content": system_prompt}] + parsed_messages
    st.download_button(
        "",
        data=json.dumps(parsed_messages, ensure_ascii=False),
        file_name=f"å¯¹è¯è®°å½•_{time.strftime("%Y-%m-%d_%H-%M-%S", time.localtime())}.json",
        icon=":material/download:",
        help="å¯¼å‡ºå¯¹è¯è®°å½•")
        

if user_input := st.chat_input(placeholder="å‘é€æ¶ˆæ¯"):
    chatbox_placeholder.empty()
    chat_box.chat_message("user").write(user_input)

    input_messages = [HumanMessage(user_input)]
    output = st.session_state.app.stream(
        {"messages": input_messages, "system_prompt": system_prompt},
        config = {"configurable": st.session_state.model_config},
        stream_mode=["messages", "values"]
    )

    with chat_box.chat_message("ai"):
        run_chat(output)

# with st.expander("debug"):
#     st.write(st.session_state.messages)
# é¡µé¢ç»“æŸ
